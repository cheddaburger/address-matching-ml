{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05828ddd",
   "metadata": {},
   "source": [
    "# Address Matching with ML (Sanitized, Refactored)\n",
    "\n",
    "This notebook trains an **scikit-learn** model to match **vendor premises** to **site inventory** records using:\n",
    "\n",
    "- Address text similarity features (street/city/state)\n",
    "- House number difference\n",
    "- Optional fuzzy score\n",
    "- **Geo-distance features (miles)** using **LAT/LON**\n",
    "- A public geocoding API (**US Census Geocoder**) to generate **vendor** LAT/LON  \n",
    "  *(site inventory is assumed to already include LAT/LON)*\n",
    "\n",
    "> Notes\n",
    "> - This is company-agnostic: no proprietary paths, names, or identifiers.\n",
    "> - Youâ€™ll need to map your real column names in `COLUMN_MAP`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config (edit this) ---\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "SITE_FILE  = DATA_DIR / \"site_inventory.csv\"\n",
    "VENDOR_FILE = DATA_DIR / \"vendor_premises.csv\"\n",
    "\n",
    "# Optional: labeled training pairs (recommended)\n",
    "# Must contain: vendor_id, site_id, label (1 match / 0 non-match)\n",
    "LABELED_PAIRS_FILE = DATA_DIR / \"labeled_pairs.csv\"  # set to None if you don't have labels\n",
    "\n",
    "# Column mapping (update to your real columns)\n",
    "COLUMN_MAP = {\n",
    "    \"site\": {\n",
    "        \"id\": \"site_id\",\n",
    "        \"street\": \"street\",\n",
    "        \"city\": \"city\",\n",
    "        \"state\": \"state\",\n",
    "        \"zip\": \"zip\",\n",
    "        \"lat\": \"latitude\",     # assumed present in site inventory\n",
    "        \"lon\": \"longitude\",\n",
    "    },\n",
    "    \"vendor\": {\n",
    "        \"id\": \"vendor_id\",\n",
    "        \"street\": \"street\",\n",
    "        \"city\": \"city\",\n",
    "        \"state\": \"state\",\n",
    "        \"zip\": \"zip\",\n",
    "        # vendor lat/lon will be generated via geocoding\n",
    "    }\n",
    "}\n",
    "\n",
    "# Candidate generation settings\n",
    "TOP_K_PER_VENDOR = 20          # number of candidate sites kept per vendor after blocking + quick scoring\n",
    "FUZZY_QUICK_THRESHOLD = 70     # quick filter before feature computation (keep loose; model does final decision)\n",
    "\n",
    "# Model settings\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538fa41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cda1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# If you have rapidfuzz installed, it's faster/better than difflib\n",
    "try:\n",
    "    from rapidfuzz.fuzz import token_sort_ratio\n",
    "except Exception:\n",
    "    token_sort_ratio = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12903279",
   "metadata": {},
   "source": [
    "## Helpers (text + geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text normalization helpers ---\n",
    "def normalize_text(x: object) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_zip(x: object) -> str:\n",
    "    s = normalize_text(x)\n",
    "    s = re.sub(r\"[^0-9]\", \"\", s)\n",
    "    return (s[:5] if len(s) >= 5 else s).zfill(5) if s else \"\"\n",
    "\n",
    "def extract_house_number(street: str) -> float:\n",
    "    street = normalize_text(street)\n",
    "    m = re.match(r\"^(\\d+)\", street)\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "def street_core(street: str) -> str:\n",
    "    # Drop house number; keep core tokens\n",
    "    s = normalize_text(street)\n",
    "    s = re.sub(r\"^\\d+\\s*\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    a, b = normalize_text(a), normalize_text(b)\n",
    "    if not a and not b:\n",
    "        return 100.0\n",
    "    if token_sort_ratio:\n",
    "        return float(token_sort_ratio(a, b))\n",
    "    # fallback: simple overlap proxy (0..100)\n",
    "    aset, bset = set(a.split()), set(b.split())\n",
    "    if not aset and not bset:\n",
    "        return 100.0\n",
    "    if not aset or not bset:\n",
    "        return 0.0\n",
    "    return 100.0 * (len(aset & bset) / max(1, len(aset | bset)))\n",
    "\n",
    "# --- Geo helpers ---\n",
    "def haversine_miles(lat1, lon1, lat2, lon2) -> float:\n",
    "    try:\n",
    "        lat1, lon1, lat2, lon2 = map(float, [lat1, lon1, lat2, lon2])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    # Earth radius in miles\n",
    "    R = 3958.7613\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ad3d6",
   "metadata": {},
   "source": [
    "## Public geocoding for vendor premises (with cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vendor geocoding (public API: US Census Geocoder) ---\n",
    "CACHE_PATH = CACHE_DIR / \"vendor_geocode_cache.parquet\"\n",
    "\n",
    "def make_cache_key(street, city, state, zip5) -> str:\n",
    "    return f\"{normalize_text(street)}|{normalize_text(city)}|{normalize_text(state)}|{normalize_zip(zip5)}\"\n",
    "\n",
    "def load_cache() -> pd.DataFrame:\n",
    "    if CACHE_PATH.exists():\n",
    "        return pd.read_parquet(CACHE_PATH)\n",
    "    return pd.DataFrame(columns=[\"cache_key\", \"lat\", \"lon\", \"status\", \"queried_at\", \"source\"])\n",
    "\n",
    "def save_cache(df: pd.DataFrame) -> None:\n",
    "    df.to_parquet(CACHE_PATH, index=False)\n",
    "\n",
    "def upsert_cache(cache_df: pd.DataFrame, new_rows_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if new_rows_df is None or new_rows_df.empty:\n",
    "        return cache_df\n",
    "    out = pd.concat([cache_df, new_rows_df], ignore_index=True)\n",
    "    out = out.drop_duplicates(\"cache_key\", keep=\"last\")\n",
    "    return out\n",
    "\n",
    "def geocode_one_census(street, city, state, zip5):\n",
    "    \"\"\"Returns (lat, lon, status) where status is OK | NO_MATCH | ERROR\"\"\"\n",
    "    addr = f\"{street}, {city}, {state} {zip5}\".strip()\n",
    "    url = \"https://geocoding.geo.census.gov/geocoder/locations/onelineaddress\"\n",
    "    params = {\"address\": addr, \"benchmark\": \"Public_AR_Current\", \"format\": \"json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        matches = data.get(\"result\", {}).get(\"addressMatches\", [])\n",
    "        if not matches:\n",
    "            return np.nan, np.nan, \"NO_MATCH\"\n",
    "        m = matches[0]\n",
    "        lon = float(m[\"coordinates\"][\"x\"])\n",
    "        lat = float(m[\"coordinates\"][\"y\"])\n",
    "        return lat, lon, \"OK\"\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, \"ERROR\"\n",
    "\n",
    "def geocode_vendor_premises(vendor_df: pd.DataFrame, column_map: dict, limit=None, sleep_s=0.15) -> pd.DataFrame:\n",
    "    \"\"\"Adds vendor_lat, vendor_lon, vendor_geocode_status using cache + Census geocoder.\"\"\"\n",
    "    v = vendor_df.copy()\n",
    "\n",
    "    v_id = column_map[\"vendor\"][\"id\"]\n",
    "    v_street = column_map[\"vendor\"][\"street\"]\n",
    "    v_city = column_map[\"vendor\"][\"city\"]\n",
    "    v_state = column_map[\"vendor\"][\"state\"]\n",
    "    v_zip = column_map[\"vendor\"][\"zip\"]\n",
    "\n",
    "    v[\"cache_key\"] = v.apply(lambda r: make_cache_key(r[v_street], r[v_city], r[v_state], r[v_zip]), axis=1)\n",
    "\n",
    "    cache = load_cache()\n",
    "    cached_keys = set(cache[\"cache_key\"].astype(str))\n",
    "    all_keys = v[\"cache_key\"].astype(str).drop_duplicates().tolist()\n",
    "    misses = [k for k in all_keys if k not in cached_keys]\n",
    "\n",
    "    if limit is not None:\n",
    "        misses = misses[:limit]\n",
    "\n",
    "    print(\"Cache misses to geocode:\", len(misses))\n",
    "\n",
    "    if misses:\n",
    "        uniq = v.drop_duplicates(\"cache_key\").set_index(\"cache_key\")\n",
    "        new_rows = []\n",
    "        for i, k in enumerate(misses, start=1):\n",
    "            r = uniq.loc[k]\n",
    "            lat, lon, status = geocode_one_census(r[v_street], r[v_city], r[v_state], r[v_zip])\n",
    "            new_rows.append({\n",
    "                \"cache_key\": k,\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"status\": status,\n",
    "                \"queried_at\": pd.Timestamp.utcnow(),\n",
    "                \"source\": \"census\"\n",
    "            })\n",
    "            if i % 25 == 0:\n",
    "                print(f\"  geocoded {i}/{len(misses)}\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "        cache = upsert_cache(cache, pd.DataFrame(new_rows))\n",
    "        save_cache(cache)\n",
    "\n",
    "    # Merge cache onto vendor records\n",
    "    v = v.merge(\n",
    "        cache[[\"cache_key\", \"lat\", \"lon\", \"status\"]],\n",
    "        on=\"cache_key\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={\"lat\":\"vendor_lat\", \"lon\":\"vendor_lon\", \"status\":\"vendor_geocode_status\"})\n",
    "\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfac5c",
   "metadata": {},
   "source": [
    "## Candidate generation and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Candidate generation + feature engineering ---\n",
    "def prepare_site_inventory(site_df: pd.DataFrame, column_map: dict) -> pd.DataFrame:\n",
    "    s = site_df.copy()\n",
    "    m = column_map[\"site\"]\n",
    "    s[\"site_id\"] = s[m[\"id\"]]\n",
    "    s[\"site_street_norm\"] = s[m[\"street\"]].map(normalize_text)\n",
    "    s[\"site_city_norm\"] = s[m[\"city\"]].map(normalize_text)\n",
    "    s[\"site_state_norm\"] = s[m[\"state\"]].map(normalize_text)\n",
    "    s[\"site_zip5\"] = s[m[\"zip\"]].map(normalize_zip)\n",
    "    s[\"site_house_num\"] = s[m[\"street\"]].map(extract_house_number)\n",
    "    s[\"site_street_core\"] = s[m[\"street\"]].map(street_core)\n",
    "    s[\"site_lat\"] = pd.to_numeric(s[m[\"lat\"]], errors=\"coerce\")\n",
    "    s[\"site_lon\"] = pd.to_numeric(s[m[\"lon\"]], errors=\"coerce\")\n",
    "    return s\n",
    "\n",
    "def prepare_vendor_premises(vendor_df: pd.DataFrame, column_map: dict) -> pd.DataFrame:\n",
    "    v = vendor_df.copy()\n",
    "    m = column_map[\"vendor\"]\n",
    "    v[\"vendor_id\"] = v[m[\"id\"]]\n",
    "    v[\"vendor_street_norm\"] = v[m[\"street\"]].map(normalize_text)\n",
    "    v[\"vendor_city_norm\"] = v[m[\"city\"]].map(normalize_text)\n",
    "    v[\"vendor_state_norm\"] = v[m[\"state\"]].map(normalize_text)\n",
    "    v[\"vendor_zip5\"] = v[m[\"zip\"]].map(normalize_zip)\n",
    "    v[\"vendor_house_num\"] = v[m[\"street\"]].map(extract_house_number)\n",
    "    v[\"vendor_street_core\"] = v[m[\"street\"]].map(street_core)\n",
    "    return v\n",
    "\n",
    "def build_candidates(vendor_df: pd.DataFrame, site_df: pd.DataFrame, top_k_per_vendor=20) -> pd.DataFrame:\n",
    "    \"\"\"Block by (state, zip5), then keep top-k by quick street similarity.\"\"\"\n",
    "    # Block join\n",
    "    blocked = vendor_df.merge(\n",
    "        site_df,\n",
    "        left_on=[\"vendor_state_norm\",\"vendor_zip5\"],\n",
    "        right_on=[\"site_state_norm\",\"site_zip5\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"\")\n",
    "    )\n",
    "\n",
    "    # Quick score to prune pairs early\n",
    "    blocked[\"quick_street_sim\"] = blocked.apply(\n",
    "        lambda r: sim(r[\"vendor_street_norm\"], r[\"site_street_norm\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Optional: apply a loose quick filter\n",
    "    blocked = blocked[blocked[\"quick_street_sim\"] >= FUZZY_QUICK_THRESHOLD].copy()\n",
    "\n",
    "    # Keep top-k per vendor_id\n",
    "    blocked.sort_values([\"vendor_id\",\"quick_street_sim\"], ascending=[True, False], inplace=True)\n",
    "    topk = blocked.groupby(\"vendor_id\", as_index=False).head(top_k_per_vendor).copy()\n",
    "    return topk\n",
    "\n",
    "def add_features(pairs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pairs_df.copy()\n",
    "\n",
    "    # Text similarity features\n",
    "    df[\"street_sim\"] = df.apply(lambda r: sim(r[\"vendor_street_norm\"], r[\"site_street_norm\"]), axis=1)\n",
    "    df[\"city_sim\"]   = df.apply(lambda r: sim(r[\"vendor_city_norm\"],  r[\"site_city_norm\"]), axis=1)\n",
    "\n",
    "    # Flags / numeric diffs\n",
    "    df[\"state_match\"] = (df[\"vendor_state_norm\"] == df[\"site_state_norm\"]).astype(int)\n",
    "    df[\"same_street_core\"] = (df[\"vendor_street_core\"] == df[\"site_street_core\"]).astype(int)\n",
    "    df[\"house_diff\"] = (pd.to_numeric(df[\"vendor_house_num\"], errors=\"coerce\") - pd.to_numeric(df[\"site_house_num\"], errors=\"coerce\")).abs()\n",
    "\n",
    "    # Fuzzy score feature (if rapidfuzz not available, reuse street_sim)\n",
    "    if token_sort_ratio:\n",
    "        df[\"fuzzy_score\"] = df.apply(lambda r: float(token_sort_ratio(r[\"vendor_street_norm\"], r[\"site_street_norm\"])), axis=1)\n",
    "    else:\n",
    "        df[\"fuzzy_score\"] = df[\"street_sim\"]\n",
    "\n",
    "    # Geo features (requires vendor_lat/lon and site_lat/lon)\n",
    "    df[\"has_geo\"] = (\n",
    "        df[\"vendor_geocode_status\"].eq(\"OK\") &\n",
    "        df[\"vendor_lat\"].notna() & df[\"vendor_lon\"].notna() &\n",
    "        df[\"site_lat\"].notna() & df[\"site_lon\"].notna()\n",
    "    ).astype(int)\n",
    "\n",
    "    df[\"geo_miles\"] = df.apply(\n",
    "        lambda r: haversine_miles(r[\"vendor_lat\"], r[\"vendor_lon\"], r[\"site_lat\"], r[\"site_lon\"]) if r[\"has_geo\"] == 1 else np.nan,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1522a4",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea17603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model training / evaluation ---\n",
    "FEATURE_COLS = [\n",
    "    \"street_sim\", \"city_sim\",\n",
    "    \"state_match\", \"same_street_core\",\n",
    "    \"house_diff\", \"fuzzy_score\",\n",
    "    \"geo_miles\", \"has_geo\"\n",
    "]\n",
    "\n",
    "def make_model() -> Pipeline:\n",
    "    return Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=6000))\n",
    "    ])\n",
    "\n",
    "def train_model(training_df: pd.DataFrame) -> Pipeline:\n",
    "    X = training_df[FEATURE_COLS]\n",
    "    y = training_df[\"label\"].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    model = make_model()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b706cb",
   "metadata": {},
   "source": [
    "## Run end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "site_raw = pd.read_csv(SITE_FILE)\n",
    "vendor_raw = pd.read_csv(VENDOR_FILE)\n",
    "\n",
    "site_df = prepare_site_inventory(site_raw, COLUMN_MAP)\n",
    "vendor_df = prepare_vendor_premises(vendor_raw, COLUMN_MAP)\n",
    "\n",
    "# --- Geocode vendor premises (adds vendor_lat/vendor_lon) ---\n",
    "vendor_geo_df = geocode_vendor_premises(vendor_df, COLUMN_MAP)\n",
    "\n",
    "# --- Candidate pairs (blocked + top-k pruning) ---\n",
    "candidate_pairs = build_candidates(vendor_geo_df, site_df, top_k_per_vendor=TOP_K_PER_VENDOR)\n",
    "\n",
    "# --- Feature engineering (includes geo_miles + has_geo) ---\n",
    "feature_pairs = add_features(candidate_pairs)\n",
    "\n",
    "feature_pairs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ca0d9",
   "metadata": {},
   "source": [
    "## Bring in labels and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8be318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training labels (recommended) ---\n",
    "# Expect a file with: vendor_id, site_id, label (1=match, 0=no-match)\n",
    "if LABELED_PAIRS_FILE is None:\n",
    "    raise ValueError(\"Set LABELED_PAIRS_FILE to a labeled pairs CSV to train the model.\")\n",
    "\n",
    "labeled_pairs = pd.read_csv(LABELED_PAIRS_FILE)\n",
    "\n",
    "training_df = feature_pairs.merge(\n",
    "    labeled_pairs[[\"vendor_id\",\"site_id\",\"label\"]],\n",
    "    on=[\"vendor_id\",\"site_id\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Training rows:\", len(training_df))\n",
    "training_df[[\"vendor_id\",\"site_id\",\"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6eb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train + evaluate ---\n",
    "model = train_model(training_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a7c7e",
   "metadata": {},
   "source": [
    "## Score and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c07133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Score all candidate pairs + pick best per vendor ---\n",
    "feature_pairs = feature_pairs.copy()\n",
    "X_all = feature_pairs[FEATURE_COLS]\n",
    "feature_pairs[\"match_probability\"] = model.predict_proba(X_all)[:, 1]\n",
    "\n",
    "best_matches = (\n",
    "    feature_pairs.sort_values([\"vendor_id\",\"match_probability\"], ascending=[True, False])\n",
    "    .groupby(\"vendor_id\", as_index=False)\n",
    "    .head(1)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "best_matches[[\"vendor_id\",\"site_id\",\"match_probability\",\"geo_miles\",\"street_sim\",\"city_sim\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad13fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export ---\n",
    "best_out = OUTPUT_DIR / \"best_vendor_to_site_matches.csv\"\n",
    "all_out  = OUTPUT_DIR / \"scored_candidate_pairs.csv\"\n",
    "\n",
    "best_matches.to_csv(best_out, index=False)\n",
    "feature_pairs.to_csv(all_out, index=False)\n",
    "\n",
    "print(\"Wrote:\", best_out)\n",
    "print(\"Wrote:\", all_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
